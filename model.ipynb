{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 1: Setup and Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os # <-- THIS LINE FIXES THE ERROR\n",
    "import re\n",
    "\n",
    "# Prefer GPU explicitly and enable mixed precision + memory growth\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(f\"‚úÖ GPU in use: {gpus[0].name} | Mixed precision ON\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No GPU detected. Running on CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GPU setup skipped: {e}\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully.\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 2: Configuration and Hyperparameters\n",
    "\n",
    "# --- Dataset File Paths ---\n",
    "# !! Make sure these files are in the same directory as your notebook !!\n",
    "file_paths = {\n",
    "    'csic': 'csic_2010.csv',\n",
    "    'sqli': 'SQL_Injection_Dataset.csv',\n",
    "    'xss': 'XSS_dataset.csv',\n",
    "    'url': 'malicious_urls.csv',\n",
    "    'access_log': 'access.log'  # Apache/Nginx combined log\n",
    "}\n",
    "\n",
    "# Access log parsing options\n",
    "access_log_config = {\n",
    "    'combined_regex': r'^(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"(\\S+) (.*?) (HTTP\\/\\d\\.\\d)\" (\\d{3}) (\\S+) \"(.*?)\" \"(.*?)\"$',\n",
    "    # Labeling heuristic: mark 5xx or WAF-like markers as malicious; others benign\n",
    "    'malicious_status_codes': set(range(500, 600)),\n",
    "    'waf_keywords': ['attack', 'blocked', 'malicious', 'sql injection', 'xss'],\n",
    "}\n",
    "\n",
    "# Sampling/balancing options\n",
    "BALANCE_PER_SOURCE = True\n",
    "PER_SOURCE_MAX = 50000        # cap records per source (after cleaning)\n",
    "ACCESS_LOG_MAX = 25000        # cap from access.log specifically\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "MAX_FEATURES = 10000\n",
    "MAX_LEN = 250\n",
    "EMBEDDING_DIM = 128\n",
    "GRU_UNITS = 128\n",
    "\n",
    "# --- Training Parameters ---\n",
    "BATCH_SIZE = 1024                 # larger batch for GPU throughput\n",
    "EPOCHS = 5\n",
    "EARLY_STOP_PATIENCE = 2\n",
    "TRAIN_MAX_SAMPLES = 200_000       # cap total training samples for speed (None to disable)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ee15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Load Data and Split First, Then Preprocess (FIXED - No Data Leakage)\n",
    "\n",
    "# Helpers for access.log parsing\n",
    "\n",
    "def parse_access_log_line(line, cfg):\n",
    "    m = re.match(cfg['combined_regex'], line)\n",
    "    if not m:\n",
    "        return None\n",
    "    method, path, protocol = m.group(5), m.group(6), m.group(7)\n",
    "    status = int(m.group(8))\n",
    "    referer = m.group(10)\n",
    "    ua = m.group(11)\n",
    "    payload = path if path else ''\n",
    "    # Heuristic labeling: conservative to reduce bias\n",
    "    waf_hit = any(k in line.lower() for k in cfg['waf_keywords'])\n",
    "    label = 1 if (status in cfg['malicious_status_codes'] or waf_hit) else 0\n",
    "    return {'payload': payload, 'label': label, 'status': status, 'method': method, 'referer': referer, 'ua': ua}\n",
    "\n",
    "# First, try to load the master dataset\n",
    "df_master = None\n",
    "try:\n",
    "    master_dataset_path = 'prepared_dataset/master_web_attack_dataset.csv'\n",
    "    if os.path.exists(master_dataset_path):\n",
    "        print(f\"\\nüîÑ Loading prepared master dataset from {master_dataset_path}...\")\n",
    "        df_master = pd.read_csv(master_dataset_path)\n",
    "        print(f\"‚úÖ Master dataset loaded successfully!\")\n",
    "    else:\n",
    "        print(\"\\nüîÑ Loading and processing individual datasets...\")\n",
    "        df_csic = pd.read_csv(file_paths['csic'])\n",
    "        df_sqli = pd.read_csv(file_paths['sqli'])\n",
    "        df_xss = pd.read_csv(file_paths['xss'])\n",
    "        df_url = pd.read_csv(file_paths['url'])\n",
    "        print(\"... CSV datasets loaded successfully.\")\n",
    "\n",
    "        # Optionally include access.log if present (with cap)\n",
    "        access_rows = []\n",
    "        if os.path.exists(file_paths['access_log']):\n",
    "            print(\"\\nüîÑ Parsing access.log...\")\n",
    "            with open(file_paths['access_log'], 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    row = parse_access_log_line(line.strip(), access_log_config)\n",
    "                    if row and len(str(row['payload'])) > 1:\n",
    "                        access_rows.append(row)\n",
    "                        if len(access_rows) >= ACCESS_LOG_MAX:\n",
    "                            break\n",
    "            print(f\"Parsed {len(access_rows):,} log lines (capped at {ACCESS_LOG_MAX}).\")\n",
    "            df_access = pd.DataFrame(access_rows)[['payload','label']]\n",
    "        else:\n",
    "            df_access = pd.DataFrame(columns=['payload','label'])\n",
    "\n",
    "        # Standardize CSIC 2010\n",
    "        df1 = df_csic[['content', 'classification']].copy()\n",
    "        df1.rename(columns={'content': 'payload', 'classification': 'label'}, inplace=True)\n",
    "        df1['label'] = df1['label'].apply(lambda x: 1 if str(x).lower() == 'anomalous' else 0)\n",
    "\n",
    "        # Standardize SQL Injection\n",
    "        df2 = df_sqli[['Query', 'Label']].copy()\n",
    "        df2.rename(columns={'Query': 'payload', 'Label': 'label'}, inplace=True)\n",
    "\n",
    "        # Standardize Cross-Site Scripting (XSS)\n",
    "        df3 = df_xss[['Sentence', 'Label']].copy()\n",
    "        df3.rename(columns={'Sentence': 'payload', 'Label': 'label'}, inplace=True)\n",
    "\n",
    "        # Standardize Malicious URLs\n",
    "        df4 = df_url[['url', 'type']].copy()\n",
    "        df4.rename(columns={'url': 'payload', 'type': 'label'}, inplace=True)\n",
    "        df4['label'] = df4['label'].apply(lambda x: 0 if x == 'benign' else 1)\n",
    "\n",
    "        # Balance per source if enabled\n",
    "        if BALANCE_PER_SOURCE:\n",
    "            def cap(df, maxn):\n",
    "                if df.empty:\n",
    "                    return df\n",
    "                n = min(len(df), maxn)\n",
    "                return df.sample(n=n, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "            df1 = cap(df1, PER_SOURCE_MAX)\n",
    "            df2 = cap(df2, PER_SOURCE_MAX)\n",
    "            df3 = cap(df3, PER_SOURCE_MAX)\n",
    "            df4 = cap(df4, PER_SOURCE_MAX)\n",
    "            df_access = cap(df_access, min(ACCESS_LOG_MAX, PER_SOURCE_MAX))\n",
    "\n",
    "            # Equalize to the smallest available among sources\n",
    "            sizes = [len(df) for df in [df1, df2, df3, df4, df_access] if len(df) > 0]\n",
    "            if sizes:\n",
    "                target = min(sizes)\n",
    "                df1 = df1.sample(n=min(len(df1), target), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "                df2 = df2.sample(n=min(len(df2), target), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "                df3 = df3.sample(n=min(len(df3), target), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "                df4 = df4.sample(n=min(len(df4), target), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "                df_access = df_access.sample(n=min(len(df_access), target), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "        # Combine, Clean, and Shuffle\n",
    "        df_combined = pd.concat([df1, df2, df3, df4, df_access], ignore_index=True)\n",
    "        df_combined.dropna(inplace=True)\n",
    "        df_combined = df_combined[df_combined['payload'].astype(str).str.len() > 1]\n",
    "        df_master = df_combined.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "        print(\"\\n--- ‚úÖ Master Dataset Created Successfully (with access.log if available) ---\")\n",
    "        os.makedirs('prepared_dataset', exist_ok=True)\n",
    "        df_master.to_csv(master_dataset_path, index=False)\n",
    "        print(f\"\\nüíæ Master dataset saved to: '{master_dataset_path}'\")\n",
    "\n",
    "except (FileNotFoundError, KeyError) as e:\n",
    "    print(f\"\\n‚ùå ERROR during data preparation: {e}\")\n",
    "    print(\"Please check that all CSV files exist and that the column names are correct.\")\n",
    "\n",
    "if df_master is not None and not df_master.empty:\n",
    "    print(\"\\nüîÑ Splitting data into training and testing sets FIRST...\")\n",
    "    \n",
    "    payloads_raw = df_master['payload'].astype(str).values\n",
    "    labels_raw = df_master['label'].values\n",
    "    \n",
    "    X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "        payloads_raw, labels_raw,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_raw\n",
    "    )\n",
    "    \n",
    "    print(f\"Raw training samples: {len(X_train_raw):,}\")\n",
    "    print(f\"Raw testing samples:  {len(X_test_raw):,}\")\n",
    "    \n",
    "    print(\"\\nüîÑ Preprocessing data: Converting text to numerical sequences...\")\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=MAX_FEATURES, char_level=True, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(X_train_raw)\n",
    "    \n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_raw)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_raw)\n",
    "    \n",
    "    X_train = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "    X_test = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n",
    "    \n",
    "    y_train = np.asarray(y_train_raw)\n",
    "    y_test = np.asarray(y_test_raw)\n",
    "\n",
    "    # Optional cap for quicker training without leakage (applies to training split only)\n",
    "    if TRAIN_MAX_SAMPLES:\n",
    "        n = min(TRAIN_MAX_SAMPLES, X_train.shape[0])\n",
    "        rng = np.random.default_rng(RANDOM_STATE)\n",
    "        idx = rng.choice(X_train.shape[0], size=n, replace=False)\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "        print(f\"Capped training samples to: {n}\")\n",
    "    \n",
    "    print(f\"Training tensor shape (X_train): {X_train.shape}\")\n",
    "    print(f\"Training tensor shape (y_train): {y_train.shape}\")\n",
    "    print(f\"Testing tensor shape (X_test): {X_test.shape}\")\n",
    "    print(f\"Testing tensor shape (y_test): {y_test.shape}\")\n",
    "    \n",
    "    X_test_raw_stored = X_test_raw\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping preprocessing because the master dataframe was not created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94eaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b07c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: Train-Test Split Already Done (REMOVED - No longer needed)\n",
    "\n",
    "# This cell is no longer needed since we already split the data in Cell 3\n",
    "# The train-test split is now done BEFORE tokenization to prevent data leakage\n",
    "\n",
    "if 'X_train' in locals() and 'X_test' in locals():\n",
    "    print(\"‚úÖ Train-test split already completed in Cell 3 with proper data leakage prevention.\")\n",
    "    print(f\"Final training samples: {len(X_train):,}\")\n",
    "    print(f\"Final testing samples:  {len(X_test):,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data preprocessing not completed yet. Please run Cell 3 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: Data Split Already Completed (REMOVED - No longer needed)\n",
    "\n",
    "# This cell is no longer needed since we already split the data in Cell 3\n",
    "# The train-test split is now done BEFORE tokenization to prevent data leakage\n",
    "\n",
    "if 'X_train' in locals() and 'X_test' in locals():\n",
    "    print(\"‚úÖ Data split already completed in Cell 3 with proper data leakage prevention.\")\n",
    "    print(f\"Training samples: {len(X_train):,}\")\n",
    "    print(f\"Testing samples:  {len(X_test):,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data preprocessing not completed yet. Please run Cell 3 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03105401",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Build the GRU Model Architecture (Modern Practice)\n",
    "\n",
    "print(\"\\nüß† Building the GRU model...\")\n",
    "\n",
    "# -----------------------------\n",
    "# ‚úÖ Import dependencies first\n",
    "# -----------------------------\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, GRU, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# -----------------------------\n",
    "# ‚úÖ Build the GRU model using global hyperparameters from Cell 2\n",
    "# -----------------------------\n",
    "model = Sequential(name=\"GRU_Web_Threat_Detector\")\n",
    "\n",
    "# 1. Explicit Input Layer\n",
    "model.add(tf.keras.Input(shape=(MAX_LEN,)))\n",
    "\n",
    "# 2. Embedding Layer\n",
    "model.add(Embedding(input_dim=MAX_FEATURES, output_dim=EMBEDDING_DIM))\n",
    "\n",
    "# 3. GRU-based Layers\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(GRU(units=GRU_UNITS, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# -----------------------------\n",
    "# ‚úÖ Compile the model\n",
    "# -----------------------------\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# ‚úÖ Model summary\n",
    "# -----------------------------\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting optimized model training on Apple M4...\")\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# ‚úÖ Enable GPU acceleration and mixed precision (Apple Metal backend)\n",
    "tf.config.optimizer.set_jit(True)\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# ‚úÖ Sanity check: list devices\n",
    "print(\"üì¶ Available devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# Ensure required globals exist\n",
    "if 'X_train' in globals() and 'y_train' in globals() and 'model' in globals():\n",
    "    # ‚öñÔ∏è Compute class weights (helps if dataset is imbalanced)\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    total = float(len(y_train))\n",
    "    class_weight = {int(k): total / (2.0 * float(v)) for k, v in dict(zip(unique, counts)).items()}\n",
    "    print(f\"‚úÖ Class weights: {class_weight}\")\n",
    "\n",
    "    # ‚öôÔ∏è Optimizer and compile step\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=3e-4, clipnorm=1.0)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    # üß† Create efficient tf.data pipeline (GPU-friendly)\n",
    "    BATCH_SIZE = 64 if 'BATCH_SIZE' not in globals() else BATCH_SIZE\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .shuffle(buffer_size=len(y_train))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # üïí Custom callback to measure time and print metrics per epoch\n",
    "    class EpochTimer(tf.keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.epoch_times = []\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self._epoch_start = time.perf_counter()\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            duration = time.perf_counter() - self._epoch_start\n",
    "            self.epoch_times.append(duration)\n",
    "            avg = np.mean(self.epoch_times)\n",
    "            remaining = (self.params['epochs'] - (epoch + 1)) * avg\n",
    "            loss = logs.get('loss', 0)\n",
    "            acc = logs.get('accuracy', 0)\n",
    "            val_loss = logs.get('val_loss', 0)\n",
    "            val_acc = logs.get('val_accuracy', 0)\n",
    "            print(f\"Epoch {epoch+1}/{self.params['epochs']} \"\n",
    "                  f\"| ‚è± {duration:.2f}s | ETA: {remaining/60:.1f}m \"\n",
    "                  f\"| loss: {loss:.4f} | acc: {acc:.4f} \"\n",
    "                  f\"| val_loss: {val_loss:.4f} | val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    epoch_timer = EpochTimer()\n",
    "\n",
    "    # üß© Callbacks: EarlyStopping, ReduceLROnPlateau, and ModelCheckpoint\n",
    "    EARLY_STOP_PATIENCE = 3 if 'EARLY_STOP_PATIENCE' not in globals() else EARLY_STOP_PATIENCE\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=EARLY_STOP_PATIENCE, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6, verbose=1\n",
    "    )\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.keras',\n",
    "        monitor='val_loss', save_best_only=True, mode='min', verbose=1\n",
    "    )\n",
    "\n",
    "    # üöÄ Train the model\n",
    "    EPOCHS = 25 if 'EPOCHS' not in globals() else EPOCHS\n",
    "    t0 = time.perf_counter()\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.1,  # automatically reserves part of training data for validation\n",
    "        callbacks=[early_stop, reduce_lr, checkpoint, epoch_timer],\n",
    "        class_weight=class_weight,\n",
    "        verbose=1\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total_time = t1 - t0\n",
    "    epochs_run = len(history.history.get('loss', []))\n",
    "    print(f\"\\n‚úÖ Training complete in {total_time/60:.1f} min \"\n",
    "          f\"({total_time/max(1,epochs_run):.2f}s/epoch over {epochs_run} epochs).\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing data or model. Please ensure Cells 1‚Äì3 and 6 have been executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad19ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: (Optional) Hybrid Model ‚Äî Skipped by Default\n",
    "\n",
    "print(\"\\n‚ÑπÔ∏è Skipping hybrid model build (requires categorical features).\")\n",
    "\n",
    "# If you later add categorical features (X_cat_train), uncomment and import:\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.layers import Input, concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "# K.clear_session()\n",
    "# text_input = Input(shape=(MAX_LEN,), name='text_input')\n",
    "# categorical_input = Input(shape=(X_cat_train.shape[1],), name='categorical_input')\n",
    "# embedding_layer = Embedding(input_dim=MAX_FEATURES, output_dim=EMBEDDING_DIM)(text_input)\n",
    "# dropout_layer = SpatialDropout1D(0.4)(embedding_layer)\n",
    "# gru_layer = GRU(units=GRU_UNITS, dropout=0.4, recurrent_dropout=0.4)(dropout_layer)\n",
    "# dense_branch = Dense(16, activation='relu')(categorical_input)\n",
    "# merged = concatenate([gru_layer, dense_branch])\n",
    "# final_dense = Dense(64, activation='relu')(merged)\n",
    "# output_layer = Dense(1, activation='sigmoid', name='output')(final_dense)\n",
    "# model = Model(inputs=[text_input, categorical_input], outputs=output_layer)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Evaluate and Plot Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "if 'model' in globals() and 'X_test' in globals() and 'y_test' in globals():\n",
    "    print(\"\\nüß™ Evaluating on test set...\")\n",
    "    eval_metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print({name: float(value) for name, value in zip(model.metrics_names, eval_metrics)})\n",
    "\n",
    "    print(\"\\nü§î Confusion Matrix:\\n\")\n",
    "    y_prob = model.predict(X_test, verbose=0).ravel()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benign', 'Malicious'],\n",
    "        yticklabels=['Benign', 'Malicious']\n",
    "    )\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìã Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Benign', 'Malicious']))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing model or test data. Ensure previous cells have been run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 10: Visualize Training History\n",
    "\n",
    "if 'history' in globals():\n",
    "    history_dict = history.history\n",
    "    acc = history_dict.get('accuracy', [])\n",
    "    val_acc = history_dict.get('val_accuracy', [])\n",
    "    loss = history_dict.get('loss', [])\n",
    "    val_loss = history_dict.get('val_loss', [])\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, 'ro-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history found. Run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 11: Real-Time Prediction Function (UPDATED)\n",
    "\n",
    "if 'tokenizer' in globals() and 'model' in globals():\n",
    "    def predict_threat(payload_string):\n",
    "        \"\"\"\n",
    "        Takes a raw string, preprocesses it, and predicts if it's a threat.\n",
    "        Uses the tokenizer fitted on training data and the trained model.\n",
    "        \"\"\"\n",
    "        seq = tokenizer.texts_to_sequences([payload_string])\n",
    "        padded_seq = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "        prediction_prob = float(model.predict(padded_seq, verbose=0)[0][0])\n",
    "        verdict = \"üö® THREAT DETECTED üö®\" if prediction_prob > 0.5 else \"‚úÖ BENIGN ‚úÖ\"\n",
    "        print(f\"\\nInput: '{payload_string}'\")\n",
    "        print(f\"Malicious Probability: {prediction_prob:.4f}\")\n",
    "        print(f\"Verdict: {verdict}\")\n",
    "\n",
    "    print(\"\\nüïµÔ∏è--- Real-Time Threat Detection Test ---\")\n",
    "    predict_threat(\"<script>alert('xss attack');</script>\")\n",
    "    predict_threat(\"1' OR '1'='1'; --\")\n",
    "    predict_threat(\"https://www.google.com/search?q=normal+search\")\n",
    "    predict_threat(\"../../../etc/passwd\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing tokenizer or model. Ensure Cells 1‚Äì3 and 6‚Äì7 have been run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58336d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 12: User Testing (Interactive Inference)\n",
    "\n",
    "if 'tokenizer' in globals() and 'model' in globals():\n",
    "    try:\n",
    "        user_text = input(\"Enter a payload/URL to analyze: \")\n",
    "        if user_text:\n",
    "            seq = tokenizer.texts_to_sequences([user_text])\n",
    "            padded_seq = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "            prob = float(model.predict(padded_seq, verbose=0)[0][0])\n",
    "            verdict = \"üö® THREAT DETECTED\" if prob > 0.5 else \"‚úÖ SAFE\"\n",
    "            print(\"\\n--- Analysis Result ---\")\n",
    "            print(f\"Input: {user_text}\")\n",
    "            print(f\"Malicious Probability: {prob:.4f}\")\n",
    "            print(f\"Verdict: {verdict}\")\n",
    "        else:\n",
    "            print(\"No input provided.\")\n",
    "    except EOFError:\n",
    "        print(\"Interactive input not available in this environment.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing tokenizer or model. Ensure training is completed (Cells 1‚Äì7).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
